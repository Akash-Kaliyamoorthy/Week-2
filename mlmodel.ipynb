{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPfEYpddI/oLC4kCrd22Eyl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mk-_V39odO5W","executionInfo":{"status":"ok","timestamp":1762673465377,"user_tz":-330,"elapsed":4313,"user":{"displayName":"K. Akash","userId":"17315511484500320062"}},"outputId":"c333ab7d-3cf9-4818-a45d-10b83a8ebdd5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n","Requirement already satisfied: geopy in /usr/local/lib/python3.12/dist-packages (2.4.1)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n","Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n","Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.12/dist-packages (from geopy) (2.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":34}],"source":["# Install required packages\n","!pip install pandas scikit-learn nltk tensorflow transformers geopy\n","\n","import pandas as pd\n","import numpy as np\n","import re\n","import json\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","import tensorflow as tf\n","from tensorflow import keras\n","from transformers import pipeline\n","import random\n","\n","# Download NLTK data\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')"]},{"cell_type":"code","source":["# Create synthetic dataset for EV charging station queries\n","data = {\n","    'text': [\n","        \"find charging stations near me\",\n","        \"where can I charge my electric car\",\n","        \"nearest EV charging point\",\n","        \"show me charging stations in downtown\",\n","        \"EV chargers nearby\",\n","        \"find fast charging stations\",\n","        \"where is the closest charging station\",\n","        \"public charging points near my location\",\n","        \"find DC fast chargers\",\n","        \"level 2 charging stations nearby\",\n","        \"are there any charging stations open now\",\n","        \"24/7 EV charging near me\",\n","        \"charging stations with available slots\",\n","        \"find free charging stations\",\n","        \"paid EV charging points\",\n","        \"Tesla superchargers near me\",\n","        \"CCS compatible charging stations\",\n","        \"CHAdeMO charging points\",\n","        \"how far is the nearest charging station\",\n","        \"directions to EV charger\"\n","    ],\n","    'intent': [\n","        'find_stations', 'find_stations', 'find_stations', 'find_stations', 'find_stations',\n","        'find_fast_charging', 'find_stations', 'find_stations', 'find_fast_charging', 'find_stations',\n","        'check_availability', 'check_availability', 'check_availability', 'filter_free', 'filter_paid',\n","        'filter_tesla', 'filter_ccs', 'filter_chademo', 'get_distance', 'get_directions'\n","    ]\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Add more training data\n","additional_data = {\n","    'text': [\n","        \"I need to charge my EV\",\n","        \"where to plug in my electric vehicle\",\n","        \"fast charging options\",\n","        \"supercharger locations\",\n","        \"find me a charger\",\n","        \"available charging spots\",\n","        \"working charging stations\",\n","        \"EV charging near shopping mall\",\n","        \"charging stations with restaurants\",\n","        \"how to get to charging point\"\n","    ],\n","    'intent': [\n","        'find_stations', 'find_stations', 'find_fast_charging', 'filter_tesla', 'find_stations',\n","        'check_availability', 'check_availability', 'find_stations', 'filter_amenities', 'get_directions'\n","    ]\n","}\n","\n","df = pd.concat([df, pd.DataFrame(additional_data)], ignore_index=True)"],"metadata":{"id":"IBkaIoN1dsKA","executionInfo":{"status":"ok","timestamp":1762673465433,"user_tz":-330,"elapsed":50,"user":{"displayName":"K. Akash","userId":"17315511484500320062"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["class TextPreprocessor:\n","    def __init__(self):\n","        self.stop_words = set(stopwords.words('english'))\n","\n","    def clean_text(self, text):\n","        # Convert to lowercase\n","        text = text.lower()\n","\n","        # Remove special characters and digits\n","        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","\n","        # Tokenize\n","        tokens = word_tokenize(text)\n","\n","        # Remove stopwords\n","        tokens = [token for token in tokens if token not in self.stop_words]\n","\n","        return ' '.join(tokens)\n","\n","# Initialize preprocessor\n","preprocessor = TextPreprocessor()\n","\n","# Preprocess the text data\n","df['cleaned_text'] = df['text'].apply(preprocessor.clean_text)\n","\n","print(\"Sample preprocessed data:\")\n","print(df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":755},"id":"GvpuWhT_d53d","executionInfo":{"status":"error","timestamp":1762673465530,"user_tz":-330,"elapsed":85,"user":{"displayName":"K. Akash","userId":"17315511484500320062"}},"outputId":"27a6d401-85f8-4d4e-f49c-8693c344c4d3"},"execution_count":36,"outputs":[{"output_type":"error","ename":"LookupError","evalue":"\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1875850296.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Preprocess the text data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample preprocessed data:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n","\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1875850296.py\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Remove stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"]}]},{"cell_type":"code","source":["# Convert text to TF-IDF features\n","vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n","X = vectorizer.fit_transform(df['cleaned_text'])\n","\n","# Encode labels\n","intent_mapping = {intent: idx for idx, intent in enumerate(df['intent'].unique())}\n","reverse_intent_mapping = {idx: intent for intent, idx in intent_mapping.items()}\n","\n","y = df['intent'].map(intent_mapping)\n","\n","print(\"Feature matrix shape:\", X.shape)\n","print(\"Intent mapping:\", intent_mapping)"],"metadata":{"id":"y1Kys7SDfHyU","executionInfo":{"status":"aborted","timestamp":1762673465825,"user_tz":-330,"elapsed":12,"user":{"displayName":"K. Akash","userId":"17315511484500320062"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train Naive Bayes classifier\n","nb_classifier = MultinomialNB()\n","nb_classifier.fit(X_train, y_train)\n","\n","# Predict and evaluate\n","y_pred = nb_classifier.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(f\"Naive Bayes Accuracy: {accuracy:.2f}\")\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred, target_names=[reverse_intent_mapping[i] for i in sorted(reverse_intent_mapping.keys())]))"],"metadata":{"id":"-zb83lf9fWdy","executionInfo":{"status":"aborted","timestamp":1762673465834,"user_tz":-330,"elapsed":5081,"user":{"displayName":"K. Akash","userId":"17315511484500320062"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","\n","# Convert sparse matrix to array for neural network\n","X_train_dense = X_train.toarray()\n","X_test_dense = X_test.toarray()\n","\n","# Build neural network\n","nn_model = Sequential([\n","    Dense(128, activation='relu', input_shape=(X_train_dense.shape[1],)),\n","    Dropout(0.3),\n","    Dense(64, activation='relu'),\n","    Dropout(0.3),\n","    Dense(len(intent_mapping), activation='softmax')\n","])\n","\n","nn_model.compile(\n","    optimizer=Adam(learning_rate=0.001),\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","# Train the model\n","history = nn_model.fit(\n","    X_train_dense, y_train,\n","    epochs=50,\n","    batch_size=8,\n","    validation_data=(X_test_dense, y_test),\n","    verbose=1\n",")\n","\n","# Evaluate neural network\n","nn_loss, nn_accuracy = nn_model.evaluate(X_test_dense, y_test)\n","print(f\"Neural Network Accuracy: {nn_accuracy:.2f}\")"],"metadata":{"id":"fLJHl7i2fp80","executionInfo":{"status":"aborted","timestamp":1762673465845,"user_tz":-330,"elapsed":5078,"user":{"displayName":"K. Akash","userId":"17315511484500320062"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EVChargingChatbot:\n","    def __init__(self, vectorizer, model, intent_mapping, reverse_intent_mapping):\n","        self.vectorizer = vectorizer\n","        self.model = model\n","        self.intent_mapping = intent_mapping\n","        self.reverse_intent_mapping = reverse_intent_mapping\n","        self.preprocessor = TextPreprocessor()\n","\n","        # Mock charging station database\n","        self.charging_stations = [\n","            {\"name\": \"EVGo Downtown\", \"type\": \"DC Fast\", \"available\": True, \"distance\": \"0.5 miles\"},\n","            {\"name\": \"ChargePoint Mall\", \"type\": \"Level 2\", \"available\": True, \"distance\": \"1.2 miles\"},\n","            {\"name\": \"Tesla Supercharger\", \"type\": \"Supercharger\", \"available\": False, \"distance\": \"2.1 miles\"},\n","            {\"name\": \"Electrify America\", \"type\": \"DC Fast\", \"available\": True, \"distance\": \"3.0 miles\"}\n","        ]\n","\n","    def predict_intent(self, text):\n","        # Preprocess text\n","        cleaned_text = self.preprocessor.clean_text(text)\n","\n","        # Transform using TF-IDF\n","        text_vector = self.vectorizer.transform([cleaned_text])\n","\n","        # Predict intent\n","        if hasattr(self.model, 'predict_proba'):\n","            # For traditional ML models\n","            prediction = self.model.predict(text_vector)[0]\n","            confidence = np.max(self.model.predict_proba(text_vector))\n","        else:\n","            # For neural network models\n","            prediction_proba = self.model.predict(text_vector.toarray())\n","            prediction = np.argmax(prediction_proba, axis=1)[0]\n","            confidence = np.max(prediction_proba)\n","\n","        intent = self.reverse_intent_mapping[prediction]\n","        return intent, confidence\n","\n","    def get_response(self, intent, user_input):\n","        intent_responses = {\n","            'find_stations': self._handle_find_stations,\n","            'find_fast_charging': self._handle_fast_charging,\n","            'check_availability': self._handle_check_availability,\n","            'filter_free': self._handle_filter_free,\n","            'filter_paid': self._handle_filter_paid,\n","            'filter_tesla': self._handle_filter_tesla,\n","            'filter_ccs': self._handle_filter_ccs,\n","            'filter_chademo': self._handle_filter_chademo,\n","            'get_distance': self._handle_get_distance,\n","            'get_directions': self._handle_get_directions,\n","            'filter_amenities': self._handle_filter_amenities\n","        }\n","\n","        handler = intent_responses.get(intent, self._handle_unknown)\n","        return handler(user_input)\n","\n","    def _handle_find_stations(self, user_input):\n","        stations = [s for s in self.charging_stations if s['available']]\n","        response = \"I found these charging stations near you:\\n\"\n","        for station in stations[:3]:\n","            response += f\"• {station['name']} ({station['type']}) - {station['distance']} away\\n\"\n","        return response\n","\n","    def _handle_fast_charging(self, user_input):\n","        fast_stations = [s for s in self.charging_stations if 'Fast' in s['type'] or 'Supercharger' in s['type']]\n","        response = \"Fast charging stations available:\\n\"\n","        for station in fast_stations:\n","            status = \"Available\" if station['available'] else \"Occupied\"\n","            response += f\"• {station['name']} - {station['distance']} - {status}\\n\"\n","        return response\n","\n","    def _handle_check_availability(self, user_input):\n","        available_stations = [s for s in self.charging_stations if s['available']]\n","        response = f\"Found {len(available_stations)} available charging stations:\\n\"\n","        for station in available_stations:\n","            response += f\"• {station['name']} - {station['distance']}\\n\"\n","        return response\n","\n","    def _handle_filter_tesla(self, user_input):\n","        tesla_stations = [s for s in self.charging_stations if 'Tesla' in s['name']]\n","        response = \"Tesla Superchargers:\\n\"\n","        for station in tesla_stations:\n","            response += f\"• {station['name']} - {station['distance']}\\n\"\n","        return response\n","\n","    def _handle_get_distance(self, user_input):\n","        nearest = min(self.charging_stations, key=lambda x: float(x['distance'].split()[0]))\n","        return f\"The nearest charging station is {nearest['name']}, {nearest['distance']} away.\"\n","\n","    def _handle_get_directions(self, user_input):\n","        return \"I can provide directions to the nearest charging station. Please enable location services for precise navigation.\"\n","\n","    def _handle_filter_free(self, user_input):\n","        return \"Free charging stations are marked in the app. Currently, most public stations require payment.\"\n","\n","    def _handle_filter_paid(self, user_input):\n","        return \"Most public charging stations require payment. You can check pricing in the charging network apps.\"\n","\n","    def _handle_filter_ccs(self, user_input):\n","        return \"CCS compatible stations: EVGo Downtown and Electrify America support CCS connectors.\"\n","\n","    def _handle_filter_chademo(self, user_input):\n","        return \"CHAdeMO stations: EVGo Downtown supports CHAdeMO connectors.\"\n","\n","    def _handle_filter_amenities(self, user_input):\n","        return \"Charging stations with amenities: Mall locations typically have restaurants and shopping nearby.\"\n","\n","    def _handle_unknown(self, user_input):\n","        return \"I'm not sure how to help with that. I can help you find charging stations, check availability, or filter by type.\"\n","\n","    def chat(self):\n","        print(\"EV Charging Station Finder Chatbot\")\n","        print(\"Type 'quit' to exit\\n\")\n","\n","        while True:\n","            user_input = input(\"You: \")\n","            if user_input.lower() in ['quit', 'exit', 'bye']:\n","                print(\"Chatbot: Goodbye! Safe travels!\")\n","                break\n","\n","            intent, confidence = self.predict_intent(user_input)\n","            print(f\"Detected intent: {intent} (confidence: {confidence:.2f})\")\n","\n","            response = self.get_response(intent, user_input)\n","            print(f\"Chatbot: {response}\\n\")"],"metadata":{"id":"Kjrg1Xitf9uA","executionInfo":{"status":"aborted","timestamp":1762673465858,"user_tz":-330,"elapsed":5077,"user":{"displayName":"K. Akash","userId":"17315511484500320062"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize chatbot with the neural network model\n","chatbot = EVChargingChatbot(vectorizer, nn_model, intent_mapping, reverse_intent_mapping)\n","\n","# Test with sample queries\n","test_queries = [\n","    \"find charging stations\",\n","    \"where are fast chargers\",\n","    \"is there a Tesla supercharger nearby\",\n","    \"how far is the nearest station\",\n","    \"get me directions to charging point\"\n","]\n","\n","print(\"Testing Chatbot:\\n\")\n","for query in test_queries:\n","    print(f\"User: {query}\")\n","    intent, confidence = chatbot.predict_intent(query)\n","    response = chatbot.get_response(intent, query)\n","    print(f\"Bot: {response}\")\n","    print(f\"Intent: {intent}, Confidence: {confidence:.2f}\\n\")"],"metadata":{"id":"Z9SLP-H1gKw4","executionInfo":{"status":"aborted","timestamp":1762673465872,"user_tz":-330,"elapsed":5084,"user":{"displayName":"K. Akash","userId":"17315511484500320062"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Uncomment to run interactive chat session\n","# chatbot.chat()"],"metadata":{"id":"GA9TjtwRgX9q","executionInfo":{"status":"aborted","timestamp":1762673465879,"user_tz":-330,"elapsed":5080,"user":{"displayName":"K. Akash","userId":"17315511484500320062"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the trained models and vectorizer\n","import joblib\n","import pickle\n","\n","# Save neural network model\n","nn_model.save('ev_chatbot_model.h5')\n","\n","# Save vectorizer and mappings\n","with open('vectorizer.pkl', 'wb') as f:\n","    pickle.dump(vectorizer, f)\n","\n","with open('intent_mappings.pkl', 'wb') as f:\n","    pickle.dump({'intent_mapping': intent_mapping, 'reverse_mapping': reverse_intent_mapping}, f)\n","\n","print(\"Models and artifacts saved successfully!\")"],"metadata":{"id":"36MwSy2sghPQ","executionInfo":{"status":"aborted","timestamp":1762673465884,"user_tz":-330,"elapsed":5079,"user":{"displayName":"K. Akash","userId":"17315511484500320062"}}},"execution_count":null,"outputs":[]}]}